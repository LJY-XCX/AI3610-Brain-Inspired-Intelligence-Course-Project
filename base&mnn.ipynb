{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ebd6503",
      "metadata": {
        "id": "7ebd6503"
      },
      "source": [
        "# å¤§ä½œä¸šè¦æ±‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f49c1bc6",
      "metadata": {
        "id": "f49c1bc6"
      },
      "source": [
        "1.å¤§ä½œä¸šä»¥2-3äººä¸ºä¸€ç»„å®Œæˆï¼Œæäº¤ææ–™åŒ…æ‹¬PPTï¼ˆæœ€åä¸€æ¬¡è¯¾å±•ç¤ºæˆæœï¼‰+ å¯è¿è¡Œçš„jupyter notebookï¼Œæ ‡æ³¨ç›¸åº”çš„æ³¨é‡Šå¹¶ç»™å‡ºè¿è¡Œç»“æœ + æœ€ç»ˆçš„å¤§ä½œä¸šæŠ¥å‘Šï¼ˆæ­£æ–‡ä¸è¶…è¿‡5é¡µword/pdfï¼Œé™„å½•å¯é€‰ä¸é™ï¼Œéœ€ç»„å†…å„æˆå‘˜å•ç‹¬æäº¤ï¼Œå†…å®¹ä¸ºæœ¬äººåœ¨è¯¾ç¨‹å¤§ä½œä¸šä¸­çš„è´¡çŒ®ä»¥åŠå¯¹å¤§ä½œä¸šé—®é¢˜çš„æ€è€ƒ) + æäº¤åŒ…å«åˆ†å·¥æƒ…å†µåŠç»„å†…å„æˆå‘˜å·¥ä½œé‡å æ¯”çš„è¡¨æ ¼ã€‚åˆ†å·¥è¡¨æ ¼éœ€ç»„å†…æ‰€æœ‰æˆå‘˜ç­¾å­—ç¡®è®¤ï¼›\n",
        "\n",
        "2.ç¦æ­¢æŠ„è¢­ï¼Œå‘ç°é›·åŒï¼Œæ‰€æœ‰é›·åŒæäº¤åˆ†æ•°é™¤ä»¥2ï¼›\n",
        "\n",
        "3.å†™æ¸…æ¥šå¤§ä½œä¸šä¸­çš„è´¡çŒ®å’Œåˆ›æ–°ç‚¹ï¼Œè‹¥ä½¿ç”¨å¼€æºä»£ç å’Œè®ºæ–‡ä¸­çš„æ–¹æ³•ï¼Œåœ¨æŠ¥å‘Šä¸­å¿…é¡»æ³¨æ˜ï¼ˆä¸å¯ä½œä¸ºæœ¬äººåˆ›æ–°ç‚¹ï¼‰ï¼Œå‘ç°ä¸æ ‡æ³¨å¼•ç”¨ï¼Œåˆ†æ•°é™¤ä»¥2ã€‚\n",
        "\n",
        "æœ€åä¸€æ¬¡è¯¾å±•ç¤ºè¯´æ˜ï¼š 1.æ ·ä¾‹ PPTä¾‹å­ï¼šhttps://www.sohu.com/a/166633625_642762 2.å±•ç¤ºæ—¶é—´é™åˆ¶ï¼šå±•ç¤ºæ—¶é—´ä¸º6åˆ†é’Ÿè®²+2åˆ†é’ŸåŒå­¦åŠ©æ•™è€å¸ˆè‡ªç”±æé—®\n",
        "\n",
        "å¤§ä½œä¸šæŠ¥å‘Šï¼šå¼ºè°ƒä¸ªäººå¯¹é—®é¢˜çš„ç†è§£ï¼Œä»¥åŠè´¡çŒ®ï¼Œå»ºè®®å¢åŠ åœ¨æé—®åé¦ˆä¹‹åçš„æ”¹è¿›ç»“æœã€‚\n",
        "\n",
        "æœ€ç»ˆè¯„åˆ†ä¸º:30%å±•ç¤ºè¯„åˆ†+70%å¤§ä½œä¸šæŠ¥å‘Š"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c69ae0",
      "metadata": {
        "id": "95c69ae0"
      },
      "source": [
        "# é—®é¢˜æè¿°"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a06c540",
      "metadata": {
        "id": "2a06c540"
      },
      "source": [
        "æ·±åº¦ç¥ç»ç½‘ç»œé€šå¸¸é‡‡ç”¨ç‹¬ç«‹åŒåˆ†å¸ƒ(Independent-Identically)çš„å‡è®¾è¿›è¡Œè®­ç»ƒï¼Œå³å‡è®¾æµ‹è¯•æ•°æ®åˆ†å¸ƒä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒç›¸ä¼¼ã€‚ç„¶è€Œï¼Œå½“ç”¨äºå®é™…ä»»åŠ¡æ—¶ï¼Œè¿™ä¸€å‡è®¾å¹¶ä¸æˆç«‹ï¼Œå¯¼è‡´å…¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è™½ç„¶è¿™ç§æ€§èƒ½ä¸‹é™å¯¹äºäº§å“æ¨èç­‰å®½å®¹æ€§å¤§çš„åº”ç”¨æ˜¯å¯ä»¥æ¥å—çš„ï¼Œä½†åœ¨åŒ»å­¦ç­‰å®½å®¹æ€§å°çš„é¢†åŸŸä½¿ç”¨æ­¤ç±»ç³»ç»Ÿæ˜¯å±é™©çš„ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½å¯¼è‡´ä¸¥é‡äº‹æ•…ã€‚ç†æƒ³çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿåº”å°½å¯èƒ½åœ¨åˆ†å¸ƒå¤–ï¼ˆOut-of-Distributionï¼‰çš„æƒ…å†µä¸‹æœ‰è¾ƒå¼ºçš„åˆ†éƒ¨å¤–æ³›åŒ–èƒ½åŠ›ã€‚è€Œæé«˜åˆ†å¸ƒå¤–æ³›åŒ–çš„å…³é”®ç‚¹ï¼Œå°±æ˜¯å¦‚ä½•è®©æ¨¡å‹å­¦ä¹ åˆ°æ•°æ®ä¸­çš„causal featureã€‚\n",
        "\n",
        "ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼šä»¥çŒ«ç‹—äºŒåˆ†ç±»ä¸ºä¾‹ï¼Œå¦‚æœè®­ç»ƒé›†ä¸­æ‰€æœ‰ç‹—éƒ½åœ¨è‰åœ°ä¸Šï¼Œæ‰€æœ‰çš„çŒ«éƒ½åœ¨æ²™å‘ä¸Šï¼Œè€Œæµ‹è¯•é›†ä¸­æ‰€æœ‰çš„ç‹—åœ¨æ²™å‘ä¸Šï¼Œæ‰€æœ‰çš„çŒ«åœ¨è‰åœ°ä¸Šï¼Œé‚£ä¹ˆæ¨¡å‹åœ¨æ²¡æœ‰æµ‹è¯•é›†ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå¾ˆæœ‰å¯èƒ½æ ¹æ®è®­ç»ƒé›†çš„ä¿¡æ¯æŠŠè‰åœ°å’Œç‹—è”ç³»åœ¨äº†ä¸€èµ·ï¼Œæ²™å‘å’ŒçŒ«è”ç³»åœ¨äº†ä¸€èµ·ï¼Œå½“æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šæµ‹è¯•æ—¶å°†ä¼šæŠŠåœ¨æ²™å‘ä¸Šçš„ç‹—è¯¯è®¤ä¸ºæ˜¯çŒ«ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36a63b0f",
      "metadata": {
        "id": "36a63b0f"
      },
      "source": [
        "# æ•°æ®é›†(Colored MNIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b15fcfe",
      "metadata": {
        "id": "5b15fcfe"
      },
      "source": [
        "Colored MNISTæ˜¯MNISTæ‰‹å†™æ•°å­—åˆ†ç±»æ•°æ®é›†çš„å˜ä½“ï¼ŒåŒ…å«æœ‰ä¸‰ä¸ªä¸åŒçš„åŸŸï¼Œæ¯ä¸ªåŸŸåŒ…å«ä¸€ç»„ä¸ç›¸äº¤çš„çº¢è‰²æˆ–ç»¿è‰²æ•°å­—å¹¶åˆ†åˆ«ä¿å­˜ä¸ºtrain1.pt, train2.pt, test.ptã€‚è¯¥æ•°æ®é›†æ€»å…±åŒ…å«60000ä¸ªæ ·æœ¬ã€‚ \n",
        "\n",
        "åœ¨è¯¥æ•°æ®é›†ä¸­ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´å­˜åœ¨Out-of-Distributionæƒ…å†µï¼Œcolor featureå’Œæ•°å­—äº§ç”Ÿäº†spurious correlationï¼Œå³è™šå‡çš„å› æœå…³ç³»ã€‚ä»ç›´è§‚ä¸Šæ¥è¯´ï¼Œæ•°å­—çš„å½¢çŠ¶ä¸ºcausal featureï¼Œæ•°å­—çš„é¢œè‰²ä¸ºnon-causal featureã€‚\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e435fec6",
      "metadata": {
        "id": "e435fec6"
      },
      "source": [
        "# Colored MNISTæ•°æ®é›†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "8a0bc19f",
      "metadata": {
        "id": "8a0bc19f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "import torchvision.datasets.utils as dataset_utils\n",
        "\n",
        "\n",
        "def color_grayscale_arr(arr, red=True):\n",
        "    \"\"\"Converts grayscale image to either red or green\"\"\"\n",
        "    assert arr.ndim == 2\n",
        "    dtype = arr.dtype\n",
        "    h, w = arr.shape\n",
        "    arr = np.reshape(arr, [h, w, 1])\n",
        "    if red:\n",
        "        arr = np.concatenate([arr,\n",
        "                              np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
        "    else:\n",
        "        arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
        "                              arr,\n",
        "                              np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
        "    return arr\n",
        "\n",
        "\n",
        "class coloredMNIST(datasets.VisionDataset):\n",
        "    \"\"\"\n",
        "  Colored MNIST dataset for testing IRM. Prepared using procedure from https://arxiv.org/pdf/1907.02893.pdf\n",
        "\n",
        "  Args:\n",
        "    root (string): Root directory of dataset where ``ColoredMNIST/*.pt`` will exist.\n",
        "    env (string): Which environment to load. Must be 1 of 'train1', 'train2', 'test', or 'all_train'.\n",
        "    transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "      and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "    target_transform (callable, optional): A function/transform that takes in the\n",
        "      target and transforms it.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, root='./data', env='test', transform=None, target_transform=None):\n",
        "        super(coloredMNIST, self).__init__(root, transform=transform,\n",
        "                                           target_transform=target_transform)\n",
        "\n",
        "        self.prepare_colored_mnist()\n",
        "        if env in ['train1', 'train2', 'test']:\n",
        "            self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt')\n",
        "        elif env == 'all_train':\n",
        "            self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', 'train1.pt')) + \\\n",
        "                                     torch.load(os.path.join(self.root, 'ColoredMNIST', 'train2.pt'))\n",
        "        else:\n",
        "            raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, test, and all_train')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "    Args:\n",
        "        index (int): Index\n",
        "\n",
        "    Returns:\n",
        "        tuple: (image, target) where target is index of the target class.\n",
        "    \"\"\"\n",
        "        img, target, color = self.data_label_tuples[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target, color\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_label_tuples)\n",
        "\n",
        "    def prepare_colored_mnist(self):\n",
        "        colored_mnist_dir = os.path.join(self.root, 'ColoredMNIST')\n",
        "        if os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
        "                and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
        "                and os.path.exists(os.path.join(colored_mnist_dir, 'test.pt')):\n",
        "            print('Colored MNIST dataset already exists')\n",
        "            return\n",
        "\n",
        "        print('Preparing Colored MNIST')\n",
        "        train_mnist = datasets.mnist.MNIST(self.root, train=True, download=True)\n",
        "\n",
        "        train1_set = []\n",
        "        train2_set = []\n",
        "        test_set = []\n",
        "        for idx, (im, label) in enumerate(train_mnist):\n",
        "            if idx % 10000 == 0:\n",
        "                print(f'Converting image {idx}/{len(train_mnist)}')\n",
        "            im_array = np.array(im)\n",
        "\n",
        "            # Assign a binary label y to the image based on the digit\n",
        "            binary_label = 0 if label < 5 else 1\n",
        "\n",
        "            # Flip label with 25% probability\n",
        "            if np.random.uniform() < 0.25:\n",
        "                binary_label = binary_label ^ 1\n",
        "\n",
        "            # Color the image either red or green according to its possibly flipped label\n",
        "            color_red = binary_label == 0\n",
        "\n",
        "            # Flip the color with a probability e that depends on the environment\n",
        "            if idx < 20000:\n",
        "                # 20% in the first training environment\n",
        "                if np.random.uniform() < 0.2:\n",
        "                    color_red = not color_red\n",
        "            elif idx < 40000:\n",
        "                # 10% in the first training environment\n",
        "                if np.random.uniform() < 0.1:\n",
        "                    color_red = not color_red\n",
        "            else:\n",
        "                # 90% in the test environment\n",
        "                if np.random.uniform() < 0.9:\n",
        "                    color_red = not color_red\n",
        "\n",
        "            colored_arr = color_grayscale_arr(im_array, red=color_red)\n",
        "\n",
        "            # æ•°æ®ä¸­å‚¨å­˜æ ·æœ¬é¢œè‰²\n",
        "            if idx < 20000:\n",
        "                train1_set.append((Image.fromarray(colored_arr), binary_label, color_red))\n",
        "            elif idx < 40000:\n",
        "                train2_set.append((Image.fromarray(colored_arr), binary_label, color_red))\n",
        "            else:\n",
        "                test_set.append((Image.fromarray(colored_arr), binary_label, color_red))\n",
        "\n",
        "            # Debug\n",
        "            # print('original label', type(label), label)\n",
        "            # print('binary label', binary_label)\n",
        "            # print('assigned color', 'red' if color_red else 'green')\n",
        "            # plt.imshow(colored_arr)\n",
        "            # plt.show()\n",
        "            # break\n",
        "\n",
        "        #dataset_utils.makedir_exist_ok(colored_mnist_dir)\n",
        "        torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
        "        torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
        "        torch.save(test_set, os.path.join(colored_mnist_dir, 'test.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "2ea28660",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ea28660",
        "outputId": "30c5f071-9eb0-403d-f85b-07d8096148ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colored MNIST dataset already exists\n",
            "0 False\n"
          ]
        }
      ],
      "source": [
        "# ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶éœ€æ‰‹åŠ¨åˆ›å»ºdata/ColoredMNISTæ–‡ä»¶å¤¹\n",
        "D = coloredMNIST(env='test')\n",
        "img, target, color = D[5]\n",
        "img.show()\n",
        "print(target, color)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11dfed0e",
      "metadata": {
        "id": "11dfed0e"
      },
      "source": [
        "# â–¶ï¸â–¶ï¸â–¶ï¸åŸºç¡€éƒ¨åˆ†"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a20aac",
      "metadata": {
        "id": "94a20aac"
      },
      "source": [
        "### 1. è®¾è®¡ColoredMNISTæ•°æ®äºŒåˆ†ç±»çš„å› æœå›¾ï¼Œåˆç†å³å¯ã€‚å¹¶åŸºäºåé—¨å‡†åˆ™ï¼Œæ¨å¯¼ğ‘ƒ(ğ‘¦|ğ‘‘ğ‘œ(ğ‘¥)) ã€æç¤ºï¼šå› æœå›¾å¯ä»¥ä¸ºE->X, E->Y, X->Y, Eä¸ºç¯å¢ƒï¼Œæ¯”å¦‚é¢œè‰²ã€‘"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f9f8b4",
      "metadata": {
        "id": "56f9f8b4"
      },
      "source": [
        "$P(y|do(x)) = \\Sigma_{e} P(y|x,e)P(x)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be422f22",
      "metadata": {
        "id": "be422f22"
      },
      "source": [
        "### 2. åœ¨ColoredMNISTæ•°æ®ä¸Šå®ç°åŸºäºåé—¨å‡†åˆ™çš„å› æœæ¨ç†ç®—æ³•ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œï¼Œæå‡æ¨¡å‹é¢„æµ‹å‡†ç¡®åº¦ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c69d37",
      "metadata": {
        "id": "21c69d37"
      },
      "source": [
        "# 1. è¯»å–æ•°æ®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "35f83e59",
      "metadata": {
        "id": "35f83e59"
      },
      "outputs": [],
      "source": [
        "########################################â–¶ï¸###############################\n",
        "###ä¸€ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„è¯»å–Colored MNISTä¾‹å­ï¼Œè¯·è¿›ä¸€æ­¥å®Œå–„ã€‚å¯ä»¥è¿›è¡Œæ•°æ®é¢„å¤„ç†ç­‰æ“ä½œã€‚###\n",
        "#######################################################################\n",
        "\n",
        "class ColoredMNIST(datasets.VisionDataset):    \n",
        "    def __init__(self, path):\n",
        "        self.data_label = torch.load(path)\n",
        "        self.red_pics = []\n",
        "        self.green_pics = []\n",
        "        for img, target, color in self.data_label:\n",
        "            if (color):\n",
        "                self.red_pics.append((img, target))\n",
        "            else:\n",
        "                self.green_pics.append((img, target))\n",
        "        \n",
        "        # æŠŠæ ·æœ¬ä¸­çš„çº¢è‰²æ ·æœ¬æ•°è¡¥è‡³ç›¸åŒ\n",
        "        cnt = np.zeros(2)\n",
        "        for img, target in self.red_pics:\n",
        "            cnt[target] += 1\n",
        "        if (cnt[0]>cnt[1]):\n",
        "            for img, target in self.red_pics:\n",
        "                if (target==1):\n",
        "                    self.red_pics.append((img, target))\n",
        "                    cnt[1] += 1\n",
        "                    if (cnt[0] == cnt[1]):\n",
        "                        break\n",
        "        if (cnt[0]<cnt[1]):\n",
        "            for img, target in self.red_pics:\n",
        "                if (target==0):\n",
        "                    self.red_pics.append((img, target))\n",
        "                    cnt[0] += 1\n",
        "                    if (cnt[0] == cnt[1]):\n",
        "                        break    \n",
        "        \n",
        "        # æŠŠæ ·æœ¬ä¸­çš„ç»¿è‰²æ ·æœ¬æ•°è¡¥è‡³ç›¸åŒ\n",
        "        cnt = np.zeros(2)\n",
        "        for img, target in self.green_pics:\n",
        "            cnt[target] += 1\n",
        "        if (cnt[0]>cnt[1]):\n",
        "            for img, target in self.green_pics:\n",
        "                if (target==1):\n",
        "                    self.green_pics.append((img, target))\n",
        "                    cnt[1] += 1\n",
        "                    if (cnt[0] == cnt[1]):\n",
        "                        break\n",
        "        if (cnt[0]<cnt[1]):\n",
        "            for img, target in self.green_pics:\n",
        "                if (target==0):\n",
        "                    self.green_pics.append((img, target))\n",
        "                    cnt[0] += 1\n",
        "                    if (cnt[0] == cnt[1]):\n",
        "                        break\n",
        "        \n",
        "        # æŠŠPIL_imageè½¬åŒ–ä¸ºtensorï¼Œä»¥ä½¿ç”¨dataloaderæ¥åŠ è½½æ•°æ®\n",
        "        trans = transforms.ToTensor()\n",
        "        self.data = []\n",
        "        for idx, (img, target) in enumerate(self.red_pics):\n",
        "            img_tensor = trans(img) * 255\n",
        "            self.data.append((img_tensor, target))\n",
        "        for idx, (img, target) in enumerate(self.green_pics):\n",
        "            img_tensor = trans(img) * 255\n",
        "            self.data.append((img_tensor, target))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index]\n",
        "        return img, target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e057cbed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e057cbed",
        "outputId": "cf13d7f3-f1cd-47c4-e68a-fe512b981f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1001\n",
            "1123\n"
          ]
        }
      ],
      "source": [
        "train_data = ColoredMNIST('data/ColoredMNIST/train1.pt')\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_data = ColoredMNIST('data/ColoredMNIST/test.pt')\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3abdfcd9",
      "metadata": {
        "id": "3abdfcd9"
      },
      "source": [
        "# 2. å®šä¹‰æ¨¡å‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "7427d5ed",
      "metadata": {
        "id": "7427d5ed"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64*4*4, 64)\n",
        "        self.fc2 = nn.Linear(64, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 64*4*4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66eed82a",
      "metadata": {
        "id": "66eed82a"
      },
      "source": [
        "# 3. è®­ç»ƒæ¨¡å‹å¹¶è¾“å‡ºæµ‹è¯•ç»“æœ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "7682feff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7682feff",
        "outputId": "0833ab0c-7ac4-4850-90a5-f27d52adc109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6709)\n",
            "tensor(0.6915)\n",
            "tensor(0.6851)\n",
            "tensor(0.6881)\n",
            "tensor(0.6646)\n",
            "tensor(0.6767)\n",
            "tensor(0.6042)\n",
            "tensor(0.6428)\n",
            "tensor(0.6152)\n",
            "tensor(0.5951)\n"
          ]
        }
      ],
      "source": [
        "net = MyModel()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "def train_test_MyModel():\n",
        "    for epoch in range(10):\n",
        "        for idx, (img, target) in enumerate(train_loader):\n",
        "            #img += torch.normal(mean=0, std=10, size=img.shape)\n",
        "            optimizer.zero_grad()\n",
        "            output = net(img)\n",
        "            loss = loss_fn(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        correct = total = 0\n",
        "        for idx, (img, target) in enumerate(test_loader):\n",
        "            output = net(img)\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "            correct += torch.sum(pred == target)\n",
        "            total += img.shape[0]\n",
        "        print(correct / total)\n",
        "\n",
        "train_test_MyModel()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0730aac3",
      "metadata": {
        "id": "0730aac3"
      },
      "source": [
        "# â–¶ï¸â–¶ï¸â–¶ï¸æé«˜éƒ¨åˆ†"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1fab09f",
      "metadata": {
        "id": "d1fab09f"
      },
      "source": [
        "### 2. å°†ä¸Šè¿°è¿‡ç¨‹ä¸­ç”¨åˆ°çš„ç¥ç»ç½‘ç»œæ›¿æ¢ä¸ºMemristorï¼Œå¯ä½¿ç”¨MemTorchåº“ï¼Œå¹¶ç ”ç©¶æ–¹æ³•æé«˜å…¶æ³›åŒ–æ€§èƒ½ï¼›"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2A70E39NrN20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A70E39NrN20",
        "outputId": "299b4099-a15e-463c-cc23-0fcd09d14a8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: memtorch in /usr/local/lib/python3.8/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from memtorch) (1.21.6)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (from memtorch) (0.0.post1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from memtorch) (0.11.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from memtorch) (7.9.0)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from memtorch) (1.13.0+cu116)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from memtorch) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from memtorch) (1.7.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from memtorch) (0.14.0+cu116)\n",
            "Requirement already satisfied: lmfit in /usr/local/lib/python3.8/dist-packages (from memtorch) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from memtorch) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.2.0->memtorch) (4.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->memtorch) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->memtorch) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->memtorch) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->memtorch) (5.7.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->memtorch) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->memtorch) (57.4.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython->memtorch) (0.18.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->memtorch) (2.0.10)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->memtorch) (0.7.5)\n",
            "Requirement already satisfied: asteval>=0.9.28 in /usr/local/lib/python3.8/dist-packages (from lmfit->memtorch) (0.9.28)\n",
            "Requirement already satisfied: uncertainties>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from lmfit->memtorch) (3.1.7)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->memtorch) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->memtorch) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->memtorch) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->memtorch) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->memtorch) (2022.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->memtorch) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->memtorch) (2.25.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->memtorch) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->memtorch) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->memtorch) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from uncertainties>=3.1.4->lmfit->memtorch) (0.16.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->memtorch) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->memtorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->memtorch) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->memtorch) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->memtorch) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install memtorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ee4dc522",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee4dc522",
        "outputId": "b96d1d1a-08a4-4b6d-8295-41de4f373a41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patched Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1)) -> bh.Conv2d(in_channels=3, out_channels=32, kernel_size=(5, 5), stride=(1, 1), padding=(0, 0))\n",
            "Patched Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1)) -> bh.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=(1, 1), padding=(0, 0))\n",
            "Patched Linear(in_features=1024, out_features=64, bias=True) -> bh.Linear(in_features=1024, out_features=64, bias=True)\n",
            "Patched Linear(in_features=64, out_features=2, bias=True) -> bh.Linear(in_features=64, out_features=2, bias=True)\n",
            "Tuned bh.Conv2d(in_channels=3, out_channels=32, kernel_size=(5, 5), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.993118 [6651.863281, -0.001044]\n",
            "Tuned bh.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=(1, 1), padding=(0, 0)). Coefficient of determination: 0.997598 [2412.404785, -0.001807]\n",
            "Tuned bh.Linear(in_features=1024, out_features=64, bias=True). Coefficient of determination: 0.999190 [2591.459961, -0.001109]\n",
            "Tuned bh.Linear(in_features=64, out_features=2, bias=True). Coefficient of determination: 0.828139 [9406.141602, -0.079612]\n"
          ]
        }
      ],
      "source": [
        "import memtorch\n",
        "reference_memristor = memtorch.bh.memristor.VTEAM\n",
        "reference_memristor_params = {'time_series_resolution': 1e-10}\n",
        "memristor = reference_memristor(**reference_memristor_params)\n",
        "#memristor.plot_hysteresis_loop()\n",
        "#memristor.plot_bipolar_switching_behaviour()\n",
        "\n",
        "import copy\n",
        "from memtorch.mn.Module import patch_model\n",
        "from memtorch.map.Input import naive_scale\n",
        "from memtorch.map.Parameter import naive_map\n",
        "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
        "\n",
        "r_on = 1.4e4\n",
        "r_off = 5e7\n",
        "\n",
        "\n",
        "def ANN_to_MNN(model, r_on, r_off, tile_shape, ADC_resolution, failure_percentage):\n",
        "    '''\n",
        "    model : pretrained model.\n",
        "    r_on : float\n",
        "        On (minimum) resistance of the device (ohms).\n",
        "    r_off : float\n",
        "        Off (maximum) resistance of the device (ohms).\n",
        "    tile_shape : int, int\n",
        "        Tile shape to use to store weights.\n",
        "    ADC_resolution : int\n",
        "        ADC resolution (bit width). If None, quantization noise is not accounted for.\n",
        "    lrs_proportion : float\n",
        "        Proportion of devices which become stuck at a low resistance state.\n",
        "    '''\n",
        "    model_ = copy.deepcopy(model)\n",
        "    reference_memristor = memtorch.bh.memristor.VTEAM\n",
        "    reference_memristor_params = {'time_series_resolution': 1e-10, 'r_off': r_off, 'r_on': r_on}\n",
        "\n",
        "    # æ¨¡å‹ä¸­æ¯ä¸€å±‚è½¬åŒ–ä¸ºå¿†é˜»å™¨å…ƒä»¶\n",
        "    patched_model = patch_model(copy.deepcopy(model_),\n",
        "                              memristor_model=reference_memristor,\n",
        "                              memristor_model_params=reference_memristor_params,\n",
        "                              module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
        "                              mapping_routine=naive_map,\n",
        "                              transistor=True,\n",
        "                              programming_routine=None,\n",
        "                              scheme=memtorch.bh.Scheme.DoubleColumn,\n",
        "                              tile_shape=tile_shape,\n",
        "                              max_input_voltage=0.3,\n",
        "                              ADC_resolution=int(ADC_resolution),\n",
        "                              ADC_overflow_rate=0.,\n",
        "                              quant_method='linear')\n",
        "\n",
        "    # åŠ å…¥éç†æƒ³ä¿®æ­£\n",
        "    patched_model = apply_nonidealities(patched_model,\n",
        "                                        non_idealities=[memtorch.bh.nonideality.NonIdeality.DeviceFaults],\n",
        "                                        lrs_proportion=failure_percentage,\n",
        "                                        hrs_proportion=0.,\n",
        "                                        electroform_proportion=0.)\n",
        "    \n",
        "    patched_model.tune_()\n",
        "\n",
        "    return patched_model\n",
        "\n",
        "MNN = ANN_to_MNN(net, r_on=r_on, r_off=r_off, tile_shape=(128,128), ADC_resolution=8, failure_percentage=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "CRfZg9ZWvxJv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRfZg9ZWvxJv",
        "outputId": "8b16febf-0dbe-499a-cc32-129e8c701033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.5093)\n"
          ]
        }
      ],
      "source": [
        "# æµ‹è¯•æ£€éªŒMNN\n",
        "def test_MNN():\n",
        "    optimizer = optim.SGD(MNN.parameters(), lr=0.001)\n",
        "\n",
        "    correct = total = 0\n",
        "    for idx, (img, target) in enumerate(test_loader):\n",
        "        #print(torch.sum(torch.abs(img[0]-img[1])))\n",
        "        output = MNN(img)\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        correct += torch.sum(pred.cpu() == target)\n",
        "        total += img.shape[0]\n",
        "    print(correct / total)\n",
        "\n",
        "test_MNN()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ox",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10 | packaged by conda-forge | (default, May 11 2021, 06:25:23) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "9bf23f2c9428cb096e9940666b5b07c0f044b949e3805dd171aa3e94f0771d01"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
